{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Plot parameters\n",
    "sns.set()\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (4, 4)\n",
    "plt.rcParams['xtick.major.size'] = 0\n",
    "plt.rcParams['ytick.major.size'] = 0\n",
    "# Avoid inaccurate floating values (for inverse matrices in dot product for instance)\n",
    "# See https://stackoverflow.com/questions/24537791/numpy-matrix-inversion-rounding-errors\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".pquote {\n",
       "  text-align: left;\n",
       "  margin: 40px 0 40px auto;\n",
       "  width: 70%;\n",
       "  font-size: 1.5em;\n",
       "  font-style: italic;\n",
       "  display: block;\n",
       "  line-height: 1.3em;\n",
       "  color: #5a75a7;\n",
       "  font-weight: 600;\n",
       "  border-left: 5px solid rgba(90, 117, 167, .1);\n",
       "  padding-left: 6px;\n",
       "}\n",
       ".notes {\n",
       "  font-style: italic;\n",
       "  display: block;\n",
       "  margin: 40px 10%;\n",
       "}\n",
       "img + em {\n",
       "  text-align: center;\n",
       "  display: block;\n",
       "  color: gray;\n",
       "  font-size: 0.9em;\n",
       "  font-weight: 600;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".pquote {\n",
    "  text-align: left;\n",
    "  margin: 40px 0 40px auto;\n",
    "  width: 70%;\n",
    "  font-size: 1.5em;\n",
    "  font-style: italic;\n",
    "  display: block;\n",
    "  line-height: 1.3em;\n",
    "  color: #5a75a7;\n",
    "  font-weight: 600;\n",
    "  border-left: 5px solid rgba(90, 117, 167, .1);\n",
    "  padding-left: 6px;\n",
    "}\n",
    ".notes {\n",
    "  font-style: italic;\n",
    "  display: block;\n",
    "  margin: 40px 10%;\n",
    "}\n",
    "img + em {\n",
    "  text-align: center;\n",
    "  display: block;\n",
    "  color: gray;\n",
    "  font-size: 0.9em;\n",
    "  font-weight: 600;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand\\bs[1]{\\boldsymbol{#1}}\n",
    "\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this lesson, we'll introduce ourselves to an important concept for machine learning and deep learning: the norm. Norms are what we generally use to evaluate the error of our models. For instance, it is used to calculate the error between the output of a neural network and what is expected (the actual label or value). You can think of the norm as the length of a vector. It is a function that maps a vector to a positive value. Different functions can be used and we will see few examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition\n",
    "Given vectors $x$ and $y$ of length one, which are simply scalars $x$ and $y$, the most natural notion of distance between $x$ and $y$ is obtained from the absolute value. Therefore we define the distance to be $\\lvert{x − y}\\rvert$. We can therefore define a distance function for vectors that has similar properties.\n",
    "\n",
    "A function $\\lvert\\lvert \\cdot \\rvert\\rvert \\colon \\mathbb{R}^{n} \\rightarrow\n",
    " \\mathbb{R}$ is called a vector norm if it has the following properties:\n",
    "\n",
    ">$\\lvert\\lvert \\bs{x} \\rvert\\rvert \\ge 0 \\text{ for any vector } \\bs{x} \\in \\mathbb R^{n}, \\text{ and } \\lvert\\lvert \\bs{x} \\rvert\\rvert = 0 \\text{ if and only if } \\bs{x} = 0$\n",
    "\n",
    ">$\\lvert\\lvert \\alpha \\bs{x} \\rvert\\rvert = \\lvert \\alpha \\rvert \\lvert\\lvert \\bs{x} \\rvert\\rvert \\text{ for any vector } \\bs{x} \\in \\mathbb R^{n} \\text{ and any } \\alpha \\in \\mathbb R$\n",
    "\n",
    ">$\\lvert\\lvert \\bs{x} + \\bs{y} \\rvert\\rvert \\le \\lvert\\lvert \\bs{x} \\rvert\\rvert + \\lvert\\lvert \\bs{y} \\rvert\\rvert \\text { for any vectors } \\bs{x},\\bs{y} \\in \\mathbb R^{n}$\n",
    "\n",
    "That last property is called the [triangle inequality](https://en.wikipedia.org/wiki/Triangle_inequality). It should be noted that when $n = 1$, the absolute value function is a vector norm.\n",
    "\n",
    "\n",
    "#### A note on notation\n",
    ">Norms are usually represented with two horizontal bars: $\\norm{\\bs{x}}$\n",
    "\n",
    ">When we say that $x \\in \\mathbb R$, we mean that $x$ is a (one-dimensional) scalar that happens to be a real number. For example, we might have $x=1$ or $x=-12$.\n",
    "\n",
    ">On the other hand, when we say that $\\vec x \\in \\mathbb R^2$, we mean that $\\vec x$ is a two-dimensional vector whose two components are both real numbers. In other words, $\\vec x$ is an ordered pair in the Cartesian plane that has the form $(x_1, x_2)$, where $x_1,x_2 \\in \\mathbb R$. For example, we might have $\\vec x = (-1, 7)$, or $\\vec x = (\\pi, 2.54)$.\n",
    "\n",
    ">When we define a function $\\lvert\\lvert \\cdot \\rvert\\rvert \\colon \\mathbb R^2 \\to \\mathbb R$, we mean that the function $\\lvert\\lvert \\cdot \\rvert\\rvert$ maps each ordered pair (which contains two numbers as input) to a single number (as output). For example, we could define such a mapping by:\n",
    "\n",
    "$$\\lvert\\lvert (x_1, x_2) \\rvert\\rvert = 2x_1 + 3x_2$$\n",
    "\n",
    ">In this case,  the function $\\lvert\\lvert \\cdot \\rvert\\rvert$ would map vector $\\vec x = (-1, 7)$ to $2(-1) + 3(7) = 19$.\n",
    "\n",
    "\n",
    "\n",
    "# The triangle inequity\n",
    "\n",
    "In plain English, the norm of the sum of some vectors is less than or equal to the sum of the norms of these vectors.\n",
    "\n",
    "$$\n",
    "\\norm{\\bs{x}+\\bs{y}} \\leq \\norm{\\bs{x}}+\\norm{\\bs{y}}\n",
    "$$\n",
    "\n",
    "### Example 1.\n",
    "\n",
    "$$\n",
    "\\bs{x}=\n",
    "\\begin{bmatrix}\n",
    "    1 & 6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\bs{y}=\n",
    "\\begin{bmatrix}\n",
    "    4 & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\norm{\\bs{x}+\\bs{y}} = \\sqrt{(1+4)^2+(6+2)^2} = \\sqrt{89} \\approx 9.43\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\norm{\\bs{x}}+\\norm{\\bs{y}} = \\sqrt{1^2+6^2}+\\sqrt{4^2+2^2} = \\sqrt{37}+\\sqrt{20} \\approx 10.55\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check these results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 6])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, 6])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([4, 2])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 8])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.4339811320566032"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.554898485297798"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(x)+np.linalg.norm(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geometrically, this simply means that the shortest path between two points is a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11d82dda0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQIAAAD3CAYAAAD2Z1pOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCCQIhBBIEAghkCAQQgD/H46hBDQL0A+4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d6e4550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [0,0,1,6]\n",
    "y = [0,0,4,2]\n",
    "x_bis = [1,6,y[2],y[3]]\n",
    "w = [0,0,5,8]\n",
    "plt.quiver([x[0], x_bis[0], w[0]],\n",
    "           [x[1], x_bis[1], w[1]],\n",
    "           [x[2], x_bis[2], w[2]],\n",
    "           [x[3], x_bis[3], w[3]],\n",
    "           angles='xy', scale_units='xy', scale=1, color=sns.color_palette())\n",
    "# plt.rc('text', usetex=True)\n",
    "plt.xlim(-2, 6)\n",
    "plt.ylim(-2, 9)\n",
    "plt.axvline(x=0, color='grey')\n",
    "plt.axhline(y=0, color='grey')\n",
    "\n",
    "plt.text(-1, 3.5, r'$||\\vec{x}||$', color=sns.color_palette()[0], size=20)\n",
    "plt.text(2.5, 7.5, r'$||\\vec{y}||$', color=sns.color_palette()[1], size=20)\n",
    "plt.text(2, 2, r'$||\\vec{x}+\\vec{y}||$', color=sns.color_palette()[2], size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P Norms\n",
    "Once you get deep into machine learning algorithms, you'll eventually be faced with choosing between the $L^1$-norm or the $L^2$-norm for your model's **loss function**, which tries to minimize the difference between predicted and true values when estimating parameters.\n",
    "\n",
    "If you want to make an informed decision about your loss functions, then you need to become familiar with the **vector norms**.\n",
    "\n",
    "The most commonly used vector norms belong to the family of $p$-norms, or $\\ell_{p}$-norms, which are defined by:\n",
    "\n",
    "$$\\left\\| x \\right\\| _p = \\left( |x_1|^p + |x_2|^p + \\dotsb + |x_n|^p \\right) ^{1/p}$$\n",
    "\n",
    "which can be more concisely expressed with the formula:\n",
    "\n",
    "$$\\norm{\\bs{x}}_p=(\\sum_{i}^{n}|\\bs{x}_i|^p)^{1/p}$$\n",
    "\n",
    "In plain English, this is how you calculate the $p$-norm of a vector:\n",
    "\n",
    "1. Calculate the absolute value of each element\n",
    "2. Raise each of those absolute values to the power $p$\n",
    "3. Sum all of these powered absolute values\n",
    "4. Raise this result to the power $\\frac{1}{p}$\n",
    "\n",
    "Let's make this a bit more concrete by looking at some widely used $p$-norms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The $L^0$ norm\n",
    "Raise any positive number to the $0^{th}$ power and you get $1$. And since $0^{0}$ is undefined, the $L^0$ norm gives the number of non-zero elements within a vector. \n",
    "\n",
    ">Technically speaking, the $L^0$ norm isn't really a norm. If you look at calculation #4 above, you'll see that when $p=0$, you can't perform $\\frac{1}{p}$ since you can't divide by $0$. The real reason for the name $L^0$ is that it is the limit as $p\\rightarrow0$ of the $L^p$ norm:\n",
    "\n",
    "$$\\norm{\\bs{x}}_0 = \\lim_{p\\rightarrow0} \\sum^{n}_{k-1}|\\bs{x}_k|^p$$\n",
    "\n",
    "In short, The $L^0$ norm is the number of non-zero elements in a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The $L^1$ norm\n",
    "\n",
    "When $p=1$, you have the $L^1$ norm, which is simply the sum of the absolute values:\n",
    "\n",
    "$$\n",
    "\\norm{\\bs{x}}_1=\\sum_{i} |\\bs{x}_i|\n",
    "$$\n",
    "\n",
    "The $L^1$-norm is also known as [least absolute deviations (LAD)](https://en.wikipedia.org/wiki/Least_absolute_deviations). This norm is used to minimize the sum of the absolute differences $S$ between the target value $Y_i$ and the estimated values $f(x_i)$:\n",
    "\n",
    "$$S = \\sum^{n}_{i=1}|y_i-f(x_i)|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Euclidean norm ($L^2$ norm)\n",
    "\n",
    "When $p=2$, you have what's called the Euclidean norm. Why is it called the Euclidean norm? Because the $L^2$ norm gives the distance from the origin to the point $X$, a consequence of the Pythagorean theorem. \n",
    "\n",
    "$$\n",
    "\\norm{\\bs{x}}_2=(\\sum_i \\bs{x}_i^2)^{1/2}\\Leftrightarrow \\sqrt{\\sum_i \\bs{x}_i^2}\n",
    "$$\n",
    "\n",
    "The $L^2$-norm is also known as least squares and \"SRSS\", which is an acronym for the square root of the sum of squares. \n",
    "\n",
    "In machine learning, it is used to minimuze the sum of the square of the differences $S$ between the target value $Y_i$ and the estimated values $f(x_i)$:\n",
    "\n",
    "$$S = \\sum^{n}_{i=1}(y_i-f(x_i))^2$$\n",
    "\n",
    "Let's see an example of this norm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.\n",
    "\n",
    "Graphically, the Euclidean norm corresponds to the length of the vector from the origin to the point obtained by linear combination (like applying Pythagorean theorem).\n",
    "\n",
    "$$\n",
    "\\bs{u}=\n",
    "\\begin{bmatrix}\n",
    "    3 \\\\\\\\\n",
    "    4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\norm{\\bs{u}}_2 &=\\sqrt{|3|^2+|4|^2}\\\\\\\\\n",
    "&=\\sqrt{25}\\\\\\\\\n",
    "&=5\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "So the $L^2$ norm is $5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $L^2$ norm can be calculated with the `linalg.norm` function from numpy. We can check the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm([3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the graphical representation of the vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11d1d0ba8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQIAAAD7CAYAAACBpZo1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d1fce48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "u = [0,0,3,4]\n",
    "\n",
    "plt.quiver([u[0]],\n",
    "           [u[1]],\n",
    "           [u[2]],\n",
    "           [u[3]],\n",
    "           angles='xy', scale_units='xy', scale=1)\n",
    "\n",
    "plt.xlim(-2, 4)\n",
    "plt.ylim(-2, 5)\n",
    "plt.axvline(x=0, color='grey')\n",
    "plt.axhline(y=0, color='grey')\n",
    "\n",
    "plt.annotate('', xy = (3.2, 0), xytext = (3.2, 4),\n",
    "             arrowprops=dict(edgecolor='black', arrowstyle = '<->'))\n",
    "plt.annotate('', xy = (0, -0.2), xytext = (3, -0.2),\n",
    "             arrowprops=dict(edgecolor='black', arrowstyle = '<->'))\n",
    "\n",
    "plt.text(1, 2.5, r'$\\vec{u}$', size=18)\n",
    "plt.text(3.3, 2, r'$\\vec{u}_y$', size=18)\n",
    "plt.text(1.5, -1, r'$\\vec{u}_x$', size=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the vector is in a 2-dimensional space, so it's easy to visualize.\n",
    "\n",
    "But you can also apply this norm to higher dimensional spaces.\n",
    "\n",
    "$$\n",
    "u=\n",
    "\\begin{bmatrix}\n",
    "    u_1\\\\\\\\\n",
    "    u_2\\\\\\\\\n",
    "    \\cdots \\\\\\\\\n",
    "    u_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "||u||_2 = \\sqrt{u_1^2+u_2^2+\\cdots+u_n^2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The squared Euclidean norm (squared $L^2$ norm)\n",
    "\n",
    "$$\n",
    "\\sum_i|\\bs{x}_i|^2\n",
    "$$\n",
    "\n",
    "\n",
    "The squared $L^2$ norm is convenient because it removes the square root and we end up with the simple sum of every squared values of the vector. \n",
    "\n",
    "The squared Euclidean norm is widely used in machine learning partly because it can be calculated with the vector operation $\\bs{x}^\\text{T}\\bs{x}$. Operations like this yield performance gains: see [here](https://softwareengineering.stackexchange.com/questions/312445/why-does-expressing-calculations-as-matrix-multiplications-make-them-faster) and [here](https://www.quora.com/What-makes-vector-operations-faster-than-for-loops) for more details.\n",
    "\n",
    "### Example 3.\n",
    "\n",
    "$$\n",
    "\\bs{x}=\n",
    "\\begin{bmatrix}\n",
    "    2 \\\\\\\\\n",
    "    5 \\\\\\\\\n",
    "    3 \\\\\\\\\n",
    "    3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\bs{x}^\\text{T}=\n",
    "\\begin{bmatrix}\n",
    "    2 & 5 & 3 & 3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\bs{x}^\\text{T}\\bs{x}&=\n",
    "\\begin{bmatrix}\n",
    "    2 & 5 & 3 & 3\n",
    "\\end{bmatrix} \\times\n",
    "\\begin{bmatrix}\n",
    "    2 \\\\\\\\\n",
    "    5 \\\\\\\\\n",
    "    3 \\\\\\\\\n",
    "    3\n",
    "\\end{bmatrix}\\\\\\\\\n",
    "&= 2\\times 2 + 5\\times 5 + 3\\times 3 + 3\\times 3= 47\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [5],\n",
       "       [3],\n",
       "       [3]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[2], [5], [3], [3]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[47]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclideanNorm = x.T.dot(x)\n",
    "euclideanNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(x)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They're the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Derivative of the squared $L^2$ norm\n",
    "\n",
    "Another advantage of the squared $L^2$ norm is that its partial derivative is easily computed:\n",
    "\n",
    "$$\n",
    "u=\n",
    "\\begin{bmatrix}\n",
    "    u_1\\\\\\\\\n",
    "    u_2\\\\\\\\\n",
    "    \\cdots \\\\\\\\\n",
    "    u_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\norm{u}_2 = u_1^2+u_2^2+\\cdots+u_n^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\dfrac{d\\norm{u}_2}{du_1} = 2u_1\\\\\\\\\n",
    "\\dfrac{d\\norm{u}_2}{du_2} = 2u_2\\\\\\\\\n",
    "\\cdots\\\\\\\\\n",
    "\\dfrac{d\\norm{u}_2}{du_n} = 2u_n\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Derivative of the $L^2$ norm\n",
    "\n",
    "In the case of the $L^2$ norm, the derivative is more complicated and takes every elements of the vector into account:\n",
    "\n",
    "$$\n",
    "\\norm{u}_2 = \\sqrt{(u_1^2+u_2^2+\\cdots+u_n^2)} = (u_1^2+u_2^2+\\cdots+u_n^2)^{\\frac{1}{2}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\dfrac{d\\norm{u}_2}{du_1} &=\n",
    "\\dfrac{1}{2}(u_1^2+u_2^2+\\cdots+u_n^2)^{\\frac{1}{2}-1}\\cdot\n",
    "\\dfrac{d}{du_1}(u_1^2+u_2^2+\\cdots+u_n^2)\\\\\\\\\n",
    "&=\\dfrac{1}{2}(u_1^2+u_2^2+\\cdots+u_n^2)^{-\\frac{1}{2}}\\cdot\n",
    "\\dfrac{d}{du_1}(u_1^2+u_2^2+\\cdots+u_n^2)\\\\\\\\\n",
    "&=\\dfrac{1}{2}\\cdot\\dfrac{1}{(u_1^2+u_2^2+\\cdots+u_n^2)^{\\frac{1}{2}}}\\cdot\n",
    "\\dfrac{d}{du_1}(u_1^2+u_2^2+\\cdots+u_n^2)\\\\\\\\\n",
    "&=\\dfrac{1}{2}\\cdot\\dfrac{1}{(u_1^2+u_2^2+\\cdots+u_n^2)^{\\frac{1}{2}}}\\cdot\n",
    "2\\cdot u_1\\\\\\\\\n",
    "&=\\dfrac{u_1}{\\sqrt{(u_1^2+u_2^2+\\cdots+u_n^2)}}\\\\\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\dfrac{d\\norm{u}_2}{du_1} = \\dfrac{u_1}{\\sqrt{(u_1^2+u_2^2+\\cdots+u_n^2)}}\\\\\\\\\n",
    "\\dfrac{d\\norm{u}_2}{du_2} = \\dfrac{u_2}{\\sqrt{(u_1^2+u_2^2+\\cdots+u_n^2)}}\\\\\\\\\n",
    "\\cdots\\\\\\\\\n",
    "\\dfrac{d\\norm{u}_2}{du_n} = \\dfrac{u_n}{\\sqrt{(u_1^2+u_2^2+\\cdots+u_n^2)}}\\\\\\\\\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### Why is it important to easily and quickly calculate these partial derivatives?\n",
    "> calculus, you use derivatives to understand how functions change over time. In machine learning, you can apply this concept to find models with parameters that minimize prediction error.\n",
    "\n",
    ">#### Partial derivatives\n",
    ">In functions with 2 or more variables, the partial derivative is the derivative of one variable with respect to the others. If we change $x$, but hold all other variables constant, how does $f(x,z)$ change? That’s one partial derivative. The next variable is $z$. If we change $z$ but hold $x$ constant, how does $f(x,z)$ change? Once calculate all of the partial derivatices, you store them in something called a gradient, which represents the full derivative of the multivariable function.\n",
    "\n",
    ">#### Gradients\n",
    ">A gradient is a vector that stores the partial derivatives of multivariable functions. It helps us calculate the slope at a specific point on a curve for functions with multiple independent variables. In order to calculate this more complex slope, we need to isolate each variable to determine how it impacts the output on its own. To do this we iterate through each of the variables and calculate the derivative of the function after holding all other variables constant. Each iteration produces a partial derivative which we store in the gradient.\n",
    "\n",
    ">#### Gradient Descent\n",
    ">Gradient descent is  the iterative optimization algorithm for finding the minimum of a function. In machine learning, this means we're finding a model that minimizes the error of our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "One problem of the squared $L^2$ norm is that it hardly discriminates between 0 and small values because the increase of the function is slow.\n",
    "\n",
    "We can see this by graphically comparing the squared $L^2$ norm with the $L^2$ norm. The $z$-axis corresponds to the norm and the $x$- and $y$-axis correspond to two parameters. The same thing is true with more than 2 dimensions but it would be hard to visualize that!\n",
    "\n",
    "$L^2$ norm:\n",
    "\n",
    "<img src=\"images/l2-norm.png\" width=\"500\" alt=\"Representation of the L2 norm\" title=\"The L2 norm\">\n",
    "<em>The L2 norm</em>\n",
    "\n",
    "Squared $L^2$ norm:\n",
    "\n",
    "<img src=\"images/squared-l2-norm.png\" width=\"500\" alt=\"Representation of the squared L2 norm\" title=\"The squared L2 norm\">\n",
    "<em>The squared L2 norm</em>\n",
    "\n",
    "$L^1$ norm:\n",
    "\n",
    "<img src=\"images/L1-norm.png\" alt=\"Representation of the L1 norm\" title=\"The L1 norm\" width=\"500\">\n",
    "<em>The L1 norm</em>\n",
    "\n",
    "These plots were made with the help of this [website](https://academo.org/demos/3d-surface-plotter/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The max norm\n",
    "\n",
    "The $L^\\infty$ norm corresponds to the absolute value of the greatest element of the vector.\n",
    "\n",
    "$$\n",
    "\\norm{\\bs{x}}_\\infty = \\max\\limits_i|x_i|\n",
    "$$\n",
    "\n",
    "The length of a vector can be calculated using the max norm.\n",
    "\n",
    "Max norm is also used as a regularization in machine learning, such as on neural network weights, called max norm regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3, 10, 99])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = array([1, 2, 3,10,99])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.0\n"
     ]
    }
   ],
   "source": [
    "maxnorm = norm(a, inf)\n",
    "print(maxnorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix norms: the Frobenius norm\n",
    "The Frobenius norm is equivalent to taking the $L^2$ norm of a matrix after flattening it. \n",
    "\n",
    "$$\n",
    "\\norm{\\bs{A}}_F=\\sqrt{\\sum_{i,j}A^2_{i,j}}\n",
    "$$\n",
    "\n",
    ">#### Regularization\n",
    "This norm is most often used as for regularization, which is the process of introducing additional information in order to solve an [ill-posed problem](https://en.wikipedia.org/wiki/Well-posed_problem) or to prevent [overfitting](https://en.wikipedia.org/wiki/Overfitting). One particular use of regularization is in the field of classification. Empirical learning of classifiers (learning from a finite data set) is always an underdetermined problem, because in general we are trying to infer a function of any $x$ given only some examples $x_{1},x_{2},\\dots x_{n}$. Thus, in classification problems, a regularization term (or regularizer) $R(f)$ can be added to a loss function.\n",
    "\n",
    "Let's see how we can peform this operation with numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [6, 4],\n",
       "       [3, 2]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1, 2], [6, 4], [3, 2]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.3666002653407556"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the angle between two vectors.\n",
    "Suppose we are interested in finding the angle between two given vectors, $\\bs{x}$ and $\\bs{y}$.\n",
    "\n",
    "Suppose we have two $n$-dimensional vectors $\\bs{x}$ and $\\bs{y}$ as shown below:\n",
    "\n",
    "$$x=\\left( \\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\x_n \\end{array} \\right) \\textrm{ and } y=\\left( \\begin{array}{c} y_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\ y_n \\end{array} \\right)$$\n",
    "\n",
    "In a previous lesson, we defined the dot product like this:\n",
    "\n",
    "$$x \\cdot y = x_1 y_1 + x_2 y_2 + x_3 y_3 + \\cdots + x_n y_n$$\n",
    "\n",
    "We also noted that the dot product is commutative:\n",
    "\n",
    "$$\\begin{array}{rcl} \n",
    "x \\cdot y & = & x_1 y_1 + x_2 y_2 \\cdots + x_n y_n \\\\ \n",
    "& = & y_1 x_1 + y_2 x_2 \\cdots + y_n x_n \\\\ \n",
    "& = & y \\cdot x \n",
    "\\end{array}$$\n",
    "\n",
    "We also showed that the dot product distributes over vector sums:\n",
    "\n",
    "$$\\begin{array}{rcl} \n",
    "x \\cdot (y + z) & = & x_1 (y_1 + z_1) + x_2 (y_2 + z_2) + \\cdots x_n (y_n + z_n)\\\\ \n",
    "& = & x_1 y_1 + x_1 z_1 + x_2 y_2 + x_2 z_2 + \\cdots + x_n y_n + x_n z_n\\\\ \n",
    "& = & (x_1 y_1 + x_2 y_2 \\cdots + x_n y_n) + (x_1 z_1 + x_2 z_2 \\cdots + x_n z_n)\\\\ \n",
    "& = & x \\cdot y + x \\cdot z \n",
    "\\end{array}$$\n",
    "\n",
    "We also noted that for any scalar $k$, we have:\n",
    "\n",
    "$$\\begin{array}{rcl} \n",
    "(kx) \\cdot y & = & kx_1 y_1 + kx_2 y_2 + \\cdots + kx_n y_n\\\\ \n",
    "& = & k(x_1 y_1 + x_2 y_2 \\cdots + x_n y_n)\\\\ \n",
    "& = & k(x \\cdot y) \n",
    "\\end{array}$$\n",
    "\n",
    "We will use these three properties in our calculation below.\n",
    "\n",
    "But first, recall how we defined $L^2$-norm of a vector above:\n",
    "\n",
    "$$||x||_2 = \\sqrt{x_1^2 + x_2^2 + \\cdots + x_n^2}$$\n",
    "\n",
    "The $L^2$-norm is meant to be geometrically interpreted as the length of the vector, or equivalently, the distance between the points $(0,0,...,0)$ and $(x_1,x_2,...,x_n)$.\n",
    "\n",
    "Interestingly, note how the $L^2$-norm can be written in a much shorter way by invoking the dot product:\n",
    "\n",
    "$$||x||_2 = \\sqrt{x \\cdot x}$$\n",
    "\n",
    "Now, armed with the ideas of the dot product and the norm of a vector, suppose we are interested in finding the angle between two given vectors, $\\bs{x}$ and $\\bs{y}$.\n",
    "\n",
    "Although you probably don't remember the [Law of Cosines](https://en.wikipedia.org/wiki/Law_of_cosines) from your high school geometry class, we're going to use it since it is a generalization on the Pythagorean Theorem and gives us the relationship between the side lengths of an arbitrary triangle. Specifically, if a triangle has side lengths $a$, $b$, and $c$, then:\n",
    "\n",
    "$$a^2 + b^2 - 2ab\\cos \\theta = c^2$$\n",
    "\n",
    "where $\\theta$ is the angle between the sides of length $a$ and $b$.\n",
    "\n",
    "As an example, consider the triangle that can be formed from the vectors $\\bs{x}$, $\\bs{y}$, and $\\bs{x−y}$:\n",
    "\n",
    "<img src=\"images/triangle.png\" height=250 width =250 img>\n",
    "\n",
    "Applying the Law of Cosines to this triangle, we have:\n",
    "\n",
    "$$||x||_2 +||y||_2 - 2||x||_2 \\, ||y||_2\\cos \\theta = ||x-y||_2$$\n",
    "\n",
    "But this implies, using our observations about the dot product made above, that:\n",
    "\n",
    "$$\\begin{array}{rcl} \n",
    "(x \\cdot x) + (y \\cdot y) - 2||x||_2 \\, ||y||_2\\cos \\theta & = & (x-y) \\cdot (x-y)\\\\ \n",
    "& = & x \\cdot (x-y) - y \\cdot (x-y)\\\\ \n",
    "& = & (x \\cdot x) - (x \\cdot y) - (y \\cdot x) + (y \\cdot y)\\\\ \n",
    "& = & (x \\cdot x) - (x \\cdot y) - (x \\cdot y) + (y \\cdot y)\\\\ \n",
    "& = & (x \\cdot x) - 2(x \\cdot y) + (y \\cdot y)\\\\ \n",
    "\\end{array}$$\n",
    "\n",
    "Subtracting the common $(\\bs{x}\\cdot\\bs{x})$ and $(\\bs{y}\\cdot\\bs{y})$ from both sides, we find:\n",
    "\n",
    "$$- 2||x||_2 \\, ||y||_2\\cos \\theta = - 2(x \\cdot y)$$\n",
    "\n",
    "Solving for $\\cos \\theta$ tells us:\n",
    "\n",
    "$$\\cos \\theta = \\frac{x \\cdot y}{||x||_2 \\, ||y||_2}$$\n",
    "\n",
    "Rearranging, we get:\n",
    "\n",
    "$$x \\cdot y = \\norm{\\bs{x}}_2\\norm{\\bs{y}}_2\\cos\\theta$$\n",
    "\n",
    "Finally, the dot product can be expressed as\n",
    "\n",
    "$$x \\cdot y = x^\\text{T}y$$\n",
    "\n",
    "And if we substitute in $x^\\text{T}y$, we get:\n",
    "\n",
    "$$\n",
    "x^\\text{T}y = \\norm{x}_2\\cdot\\norm{y}_2\\cos\\theta\n",
    "$$\n",
    "\n",
    "Which shows how the dot product can be expressed as the product of $L^2$ norms and $\\cos\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4.\n",
    "That was a whole bunch of math, so let's try to visualize this with a simple example.\n",
    "$$\n",
    "\\bs{x}=\n",
    "\\begin{bmatrix}\n",
    "    0 \\\\\\\\\n",
    "    2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\bs{y}=\n",
    "\\begin{bmatrix}\n",
    "    2 \\\\\\\\\n",
    "    2\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQIAAAD7CAYAAACBpZo1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11dac28d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [0,0,0,2]\n",
    "y = [0,0,2,2]\n",
    "\n",
    "plt.xlim(-2, 4)\n",
    "plt.ylim(-2, 5)\n",
    "plt.axvline(x=0, color='grey', zorder=0)\n",
    "plt.axhline(y=0, color='grey', zorder=0)\n",
    "\n",
    "plt.quiver([x[0], y[0]],\n",
    "           [x[1], y[1]],\n",
    "           [x[2], y[2]],\n",
    "           [x[3], y[3]],\n",
    "           angles='xy', scale_units='xy', scale=1)\n",
    "\n",
    "plt.text(-0.5, 1, r'$\\vec{x}$', size=18)\n",
    "plt.text(1.5, 0.5, r'$\\vec{y}$', size=18)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took this example for its simplicity. As we can see, the angle $\\theta$ is equal to 45°."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\bs{x^\\text{T}y}=\n",
    "\\begin{bmatrix}\n",
    "    0 & 2\n",
    "\\end{bmatrix} \\cdot\n",
    "\\begin{bmatrix}\n",
    "    2 \\\\\\\\\n",
    "    2\n",
    "\\end{bmatrix} =\n",
    "0\\times2+2\\times2 = 4\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\norm{\\bs{x}}_2=\\sqrt{0^2+2^2}=\\sqrt{4}=2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\norm{\\bs{y}}_2=\\sqrt{2^2+2^2}=\\sqrt{8}\n",
    "$$\n",
    "\n",
    "$$\n",
    "2\\times\\sqrt{8}\\times cos(45)=4\n",
    "$$\n",
    "\n",
    "Here are the operations using numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0000000000000009"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: np.cos take the angle in radian\n",
    "np.cos(np.deg2rad(45))*2*np.sqrt(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "The proof for the dot products, norms, and angles between vectors came from [here](http://www.oxfordmathcenter.com/drupal7/node/168).\n",
    "\n",
    "This lesson requried a lot of mathematical notation. A nice cheat sheet for these symbols can be found [here](https://artofproblemsolving.com/wiki/index.php/LaTeX:Symbols#Dots)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
